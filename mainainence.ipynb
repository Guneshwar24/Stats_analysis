{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import ADASYN, SVMSMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.feature_selection import SelectFromModel, mutual_info_classif  # Corrected import\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "class AdvancedPreprocessor:\n",
    "    \"\"\"\n",
    "    Comprehensive preprocessing including feature engineering, selection, and data balancing\n",
    "    \"\"\"\n",
    "    def __init__(self, balance_strategy='SVMSMOTE', feature_selection=True):\n",
    "        self.balance_strategy = balance_strategy\n",
    "        self.feature_selection = feature_selection\n",
    "        self.pipeline = None\n",
    "        self.feature_selector = None\n",
    "        \n",
    "    def create_feature_engineer(self):\n",
    "        class FeatureEngineer:\n",
    "            def fit(self, X, y=None):\n",
    "                return self\n",
    "            \n",
    "            def transform(self, X):\n",
    "                df = X.copy()\n",
    "                # Basic engineering\n",
    "                df['temperature_differential'] = df['Process temperature'] - df['Air temperature']\n",
    "                df['power_speed_ratio'] = df['Power'] / (df['Rotational speed'] + 1)\n",
    "                \n",
    "                # Advanced engineering\n",
    "                df['thermal_stress'] = df['temperature_differential'] * df['Power']\n",
    "                df['operational_stress'] = df['Torque'] * df['Tool wear']\n",
    "                df['efficiency_index'] = df['Power'] / (df['Torque'] * df['Rotational speed'] + 1)\n",
    "                df['wear_rate'] = df['Tool wear'] / (df['operational_stress'] + 1)\n",
    "                df['temperature_stability'] = df['temperature_differential'].rolling(window=3).std()\n",
    "                \n",
    "                # Interaction features\n",
    "                df['power_wear_interaction'] = df['Power'] * df['Tool wear']\n",
    "                df['speed_torque_efficiency'] = df['Rotational speed'] / (df['Torque'] + 1)\n",
    "                \n",
    "                return df\n",
    "                \n",
    "        return FeatureEngineer()\n",
    "\n",
    "    def create_outlier_handler(self):\n",
    "        class OutlierHandler:\n",
    "            def __init__(self):\n",
    "                self.thresholds = {}\n",
    "                \n",
    "            def fit(self, X, y=None):\n",
    "                for col in X.columns:\n",
    "                    if col in ['temperature_cols']:\n",
    "                        threshold = 2.0\n",
    "                    else:\n",
    "                        threshold = 1.5\n",
    "                        \n",
    "                    Q1 = X[col].quantile(0.25)\n",
    "                    Q3 = X[col].quantile(0.75)\n",
    "                    IQR = Q3 - Q1\n",
    "                    \n",
    "                    self.thresholds[col] = {\n",
    "                        'lower': Q1 - threshold * IQR,\n",
    "                        'upper': Q3 + threshold * IQR\n",
    "                    }\n",
    "                return self\n",
    "                \n",
    "            def transform(self, X):\n",
    "                X_clean = X.copy()\n",
    "                for col in X_clean.columns:\n",
    "                    if col in self.thresholds:\n",
    "                        X_clean[col] = X_clean[col].clip(\n",
    "                            lower=self.thresholds[col]['lower'],\n",
    "                            upper=self.thresholds[col]['upper']\n",
    "                        )\n",
    "                return X_clean\n",
    "                \n",
    "        return OutlierHandler()\n",
    "\n",
    "    def create_feature_selector(self):\n",
    "        class HybridFeatureSelector:\n",
    "            def __init__(self, n_features=None):\n",
    "                self.n_features = n_features\n",
    "                self.selected_features_ = None\n",
    "                self.importance_scores_ = None\n",
    "                \n",
    "            def fit(self, X, y):\n",
    "                # Multiple importance methods\n",
    "                mi_scores = mutual_info_classif(X, y)\n",
    "                \n",
    "                rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "                rf.fit(X, y)\n",
    "                rf_importance = rf.feature_importances_\n",
    "                \n",
    "                # Normalize and combine scores\n",
    "                mi_scores_norm = mi_scores / np.sum(mi_scores)\n",
    "                rf_importance_norm = rf_importance / np.sum(rf_importance)\n",
    "                \n",
    "                self.importance_scores_ = (mi_scores_norm + rf_importance_norm) / 2\n",
    "                \n",
    "                # Select features\n",
    "                if self.n_features is None:\n",
    "                    self.n_features = len(X.columns) // 2\n",
    "                    \n",
    "                feature_indices = np.argsort(self.importance_scores_)[-self.n_features:]\n",
    "                self.selected_features_ = X.columns[feature_indices]\n",
    "                \n",
    "                return self\n",
    "                \n",
    "            def transform(self, X):\n",
    "                return X[self.selected_features_]\n",
    "                \n",
    "            def get_feature_importance(self):\n",
    "                return pd.Series(self.importance_scores_, index=self.selected_features_)\n",
    "                \n",
    "        return HybridFeatureSelector()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Validate input\n",
    "        self._validate_input(X)\n",
    "        \n",
    "        # Create preprocessing steps\n",
    "        numeric_transformer = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('outlier_handler', self.create_outlier_handler()),\n",
    "            ('scaler', PowerTransformer(standardize=True))\n",
    "        ])\n",
    "        \n",
    "        # Create main pipeline\n",
    "        pipeline_steps = [\n",
    "            ('numeric', numeric_transformer),\n",
    "            ('feature_engineer', self.create_feature_engineer())\n",
    "        ]\n",
    "        \n",
    "        if self.feature_selection:\n",
    "            self.feature_selector = self.create_feature_selector()\n",
    "            pipeline_steps.append(('feature_selector', self.feature_selector))\n",
    "            \n",
    "        if self.balance_strategy:\n",
    "            if self.balance_strategy == 'SVMSMOTE':\n",
    "                sampler = SVMSMOTE(random_state=42)\n",
    "            else:\n",
    "                sampler = ADASYN(random_state=42)\n",
    "            pipeline_steps.append(('sampler', sampler))\n",
    "            \n",
    "        self.pipeline = ImbPipeline(pipeline_steps)\n",
    "        self.pipeline.fit(X, y)\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Transform data\n",
    "        X_transformed = self.pipeline.transform(X)\n",
    "        \n",
    "        # Validate output\n",
    "        self._validate_output(X_transformed)\n",
    "        \n",
    "        return X_transformed\n",
    "\n",
    "    def _validate_input(self, X):\n",
    "        required_columns = ['Air temperature', 'Process temperature', 'Rotational speed', \n",
    "                          'Torque', 'Tool wear', 'Power']\n",
    "        assert all(col in X.columns for col in required_columns), \"Missing required columns\"\n",
    "        assert not X.isnull().any().any(), \"Input contains null values\"\n",
    "        \n",
    "    def _validate_output(self, X):\n",
    "        assert not np.any(np.isnan(X)), \"Output contains NaN values\"\n",
    "        assert not np.any(np.isinf(X)), \"Output contains infinite values\"\n",
    "\n",
    "    def get_feature_importance(self):\n",
    "        if self.feature_selector:\n",
    "            return self.feature_selector.get_feature_importance()\n",
    "        return None\n",
    "\n",
    "# Usage example:\n",
    "def prepare_data(df, test_size=0.3, random_state=42):\n",
    "    \"\"\"\n",
    "    Complete data preparation pipeline\n",
    "    \"\"\"\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Split features and target\n",
    "    X = df.drop(['Machine failure', 'kmeans_cluster', 'hierarchical_cluster', 'dbscan_cluster'], axis=1)\n",
    "    y = df['Machine failure']\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Create and fit preprocessor\n",
    "    preprocessor = AdvancedPreprocessor(\n",
    "        balance_strategy='SVMSMOTE',\n",
    "        feature_selection=True\n",
    "    )\n",
    "    \n",
    "    # Process data\n",
    "    X_train_processed = preprocessor.fit_transform(X_train, y_train)\n",
    "    X_test_processed = preprocessor.transform(X_test)\n",
    "    \n",
    "    # Get feature importance\n",
    "    feature_importance = preprocessor.get_feature_importance()\n",
    "    \n",
    "    return X_train_processed, X_test_processed, y_train, y_test, preprocessor, feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, calinski_harabasz_score, davies_bouldin_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "class AdvancedClusterAnalyzer:\n",
    "    \"\"\"\n",
    "    Comprehensive clustering analysis with multiple algorithms and advanced validation\n",
    "    \"\"\"\n",
    "    def __init__(self, random_state=42):\n",
    "        self.random_state = random_state\n",
    "        self.results = {}\n",
    "        self.best_models = {}\n",
    "        self.cluster_labels = {}\n",
    "        self.feature_importance = {}\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Fit multiple clustering algorithms and store results\n",
    "        \"\"\"\n",
    "        self.X = StandardScaler().fit_transform(X)\n",
    "        self.X_original = X\n",
    "        \n",
    "        # Perform clustering with different algorithms\n",
    "        self._perform_kmeans()\n",
    "        self._perform_dbscan()\n",
    "        self._perform_hierarchical()\n",
    "        self._analyze_feature_importance()\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def _perform_kmeans(self, max_clusters=10):\n",
    "        \"\"\"\n",
    "        Perform KMeans clustering with comprehensive evaluation\n",
    "        \"\"\"\n",
    "        kmeans_results = {\n",
    "            'silhouette_scores': [],\n",
    "            'calinski_scores': [],\n",
    "            'davies_scores': [],\n",
    "            'inertia': [],\n",
    "            'models': []\n",
    "        }\n",
    "        \n",
    "        for n_clusters in range(2, max_clusters + 1):\n",
    "            kmeans = KMeans(\n",
    "                n_clusters=n_clusters,\n",
    "                random_state=self.random_state,\n",
    "                n_init=10\n",
    "            )\n",
    "            labels = kmeans.fit_predict(self.X)\n",
    "            \n",
    "            # Calculate clustering metrics\n",
    "            kmeans_results['silhouette_scores'].append(silhouette_score(self.X, labels))\n",
    "            kmeans_results['calinski_scores'].append(calinski_harabasz_score(self.X, labels))\n",
    "            kmeans_results['davies_scores'].append(davies_bouldin_score(self.X, labels))\n",
    "            kmeans_results['inertia'].append(kmeans.inertia_)\n",
    "            kmeans_results['models'].append(kmeans)\n",
    "        \n",
    "        # Find optimal number of clusters\n",
    "        optimal_n_clusters = self._find_optimal_clusters(kmeans_results)\n",
    "        self.best_models['kmeans'] = kmeans_results['models'][optimal_n_clusters-2]\n",
    "        self.cluster_labels['kmeans'] = self.best_models['kmeans'].labels_\n",
    "        self.results['kmeans'] = kmeans_results\n",
    "        \n",
    "    def _perform_dbscan(self):\n",
    "        \"\"\"\n",
    "        Perform DBSCAN clustering with parameter optimization\n",
    "        \"\"\"\n",
    "        from sklearn.neighbors import NearestNeighbors\n",
    "        \n",
    "        # Determine optimal eps using k-distance graph\n",
    "        neigh = NearestNeighbors(n_neighbors=2)\n",
    "        nbrs = neigh.fit(self.X)\n",
    "        distances, indices = nbrs.kneighbors(self.X)\n",
    "        distances = np.sort(distances[:, 1])\n",
    "        \n",
    "        # Find knee point for optimal eps\n",
    "        from kneed import KneeLocator\n",
    "        knee_locator = KneeLocator(\n",
    "            range(len(distances)), distances,\n",
    "            curve='convex', direction='increasing'\n",
    "        )\n",
    "        optimal_eps = distances[knee_locator.knee]\n",
    "        \n",
    "        dbscan_results = {\n",
    "            'eps_values': [],\n",
    "            'n_clusters': [],\n",
    "            'silhouette_scores': [],\n",
    "            'noise_points': []\n",
    "        }\n",
    "        \n",
    "        eps_range = np.linspace(optimal_eps * 0.5, optimal_eps * 1.5, 20)\n",
    "        \n",
    "        for eps in eps_range:\n",
    "            dbscan = DBSCAN(eps=eps, min_samples=5)\n",
    "            labels = dbscan.fit_predict(self.X)\n",
    "            \n",
    "            if len(np.unique(labels)) > 1:  # More than just noise\n",
    "                dbscan_results['eps_values'].append(eps)\n",
    "                dbscan_results['n_clusters'].append(len(np.unique(labels[labels != -1])))\n",
    "                dbscan_results['silhouette_scores'].append(silhouette_score(self.X, labels))\n",
    "                dbscan_results['noise_points'].append(np.sum(labels == -1))\n",
    "        \n",
    "        # Select optimal eps based on silhouette score\n",
    "        optimal_idx = np.argmax(dbscan_results['silhouette_scores'])\n",
    "        optimal_eps = dbscan_results['eps_values'][optimal_idx]\n",
    "        \n",
    "        # Fit final DBSCAN model\n",
    "        self.best_models['dbscan'] = DBSCAN(eps=optimal_eps, min_samples=5)\n",
    "        self.cluster_labels['dbscan'] = self.best_models['dbscan'].fit_predict(self.X)\n",
    "        self.results['dbscan'] = dbscan_results\n",
    "        \n",
    "    def _perform_hierarchical(self, max_clusters=10):\n",
    "        \"\"\"\n",
    "        Perform Hierarchical clustering with different linkage methods\n",
    "        \"\"\"\n",
    "        linkage_methods = ['ward', 'complete', 'average']\n",
    "        hierarchical_results = {method: {} for method in linkage_methods}\n",
    "        \n",
    "        for method in linkage_methods:\n",
    "            scores = {\n",
    "                'silhouette_scores': [],\n",
    "                'calinski_scores': [],\n",
    "                'davies_scores': [],\n",
    "                'models': []\n",
    "            }\n",
    "            \n",
    "            for n_clusters in range(2, max_clusters + 1):\n",
    "                hierarchical = AgglomerativeClustering(\n",
    "                    n_clusters=n_clusters,\n",
    "                    linkage=method\n",
    "                )\n",
    "                labels = hierarchical.fit_predict(self.X)\n",
    "                \n",
    "                scores['silhouette_scores'].append(silhouette_score(self.X, labels))\n",
    "                scores['calinski_scores'].append(calinski_harabasz_score(self.X, labels))\n",
    "                scores['davies_scores'].append(davies_bouldin_score(self.X, labels))\n",
    "                scores['models'].append(hierarchical)\n",
    "                \n",
    "            hierarchical_results[method] = scores\n",
    "            \n",
    "        # Find best method and number of clusters\n",
    "        best_method = max(linkage_methods, \n",
    "                         key=lambda m: max(hierarchical_results[m]['silhouette_scores']))\n",
    "        optimal_n_clusters = np.argmax(hierarchical_results[best_method]['silhouette_scores']) + 2\n",
    "        \n",
    "        self.best_models['hierarchical'] = AgglomerativeClustering(\n",
    "            n_clusters=optimal_n_clusters,\n",
    "            linkage=best_method\n",
    "        )\n",
    "        self.cluster_labels['hierarchical'] = self.best_models['hierarchical'].fit_predict(self.X)\n",
    "        self.results['hierarchical'] = hierarchical_results\n",
    "        \n",
    "    def _analyze_feature_importance(self):\n",
    "        \"\"\"\n",
    "        Analyze feature importance for clustering results\n",
    "        \"\"\"\n",
    "        from sklearn.ensemble import RandomForestClassifier\n",
    "        \n",
    "        for method in self.cluster_labels:\n",
    "            rf = RandomForestClassifier(n_estimators=100, random_state=self.random_state)\n",
    "            rf.fit(self.X_original, self.cluster_labels[method])\n",
    "            \n",
    "            self.feature_importance[method] = pd.Series(\n",
    "                rf.feature_importances_,\n",
    "                index=self.X_original.columns\n",
    "            ).sort_values(ascending=False)\n",
    "            \n",
    "    def plot_results(self, figsize=(20, 15)):\n",
    "        \"\"\"\n",
    "        Comprehensive visualization of clustering results\n",
    "        \"\"\"\n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Plot KMeans results\n",
    "        if 'kmeans' in self.results:\n",
    "            plt.subplot(331)\n",
    "            plt.plot(range(2, len(self.results['kmeans']['silhouette_scores']) + 2),\n",
    "                    self.results['kmeans']['silhouette_scores'])\n",
    "            plt.title('KMeans - Silhouette Scores')\n",
    "            \n",
    "            plt.subplot(332)\n",
    "            plt.plot(range(2, len(self.results['kmeans']['inertia']) + 2),\n",
    "                    self.results['kmeans']['inertia'])\n",
    "            plt.title('KMeans - Elbow Curve')\n",
    "            \n",
    "        # Plot DBSCAN results\n",
    "        if 'dbscan' in self.results:\n",
    "            plt.subplot(334)\n",
    "            plt.plot(self.results['dbscan']['eps_values'],\n",
    "                    self.results['dbscan']['silhouette_scores'])\n",
    "            plt.title('DBSCAN - Silhouette Scores vs Eps')\n",
    "            \n",
    "            plt.subplot(335)\n",
    "            plt.plot(self.results['dbscan']['eps_values'],\n",
    "                    self.results['dbscan']['noise_points'])\n",
    "            plt.title('DBSCAN - Noise Points vs Eps')\n",
    "            \n",
    "        # Plot Hierarchical results\n",
    "        if 'hierarchical' in self.results:\n",
    "            plt.subplot(337)\n",
    "            for method in self.results['hierarchical']:\n",
    "                plt.plot(range(2, len(self.results['hierarchical'][method]['silhouette_scores']) + 2),\n",
    "                        self.results['hierarchical'][method]['silhouette_scores'],\n",
    "                        label=method)\n",
    "            plt.title('Hierarchical - Silhouette Scores')\n",
    "            plt.legend()\n",
    "            \n",
    "        # Plot feature importance\n",
    "        plt.subplot(333)\n",
    "        if self.feature_importance:\n",
    "            self.feature_importance['kmeans'].plot(kind='bar')\n",
    "            plt.title('Feature Importance - KMeans')\n",
    "            plt.xticks(rotation=45)\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def get_cluster_summary(self):\n",
    "        \"\"\"\n",
    "        Generate summary statistics for each clustering method\n",
    "        \"\"\"\n",
    "        summary = {}\n",
    "        \n",
    "        for method in self.cluster_labels:\n",
    "            labels = self.cluster_labels[method]\n",
    "            n_clusters = len(np.unique(labels[labels != -1]))\n",
    "            \n",
    "            summary[method] = {\n",
    "                'n_clusters': n_clusters,\n",
    "                'silhouette_score': silhouette_score(self.X, labels) if n_clusters > 1 else None,\n",
    "                'calinski_score': calinski_harabasz_score(self.X, labels) if n_clusters > 1 else None,\n",
    "                'davies_score': davies_bouldin_score(self.X, labels) if n_clusters > 1 else None,\n",
    "                'cluster_sizes': pd.Series(labels).value_counts().to_dict()\n",
    "            }\n",
    "            \n",
    "        return pd.DataFrame(summary).T\n",
    "\n",
    "# Usage example:\n",
    "def perform_clustering_analysis(X):\n",
    "    \"\"\"\n",
    "    Perform comprehensive clustering analysis\n",
    "    \"\"\"\n",
    "    analyzer = AdvancedClusterAnalyzer()\n",
    "    analyzer.fit(X)\n",
    "    \n",
    "    # Plot results\n",
    "    analyzer.plot_results()\n",
    "    \n",
    "    # Get clustering summary\n",
    "    summary = analyzer.get_cluster_summary()\n",
    "    print(\"\\nClustering Summary:\")\n",
    "    print(summary)\n",
    "    \n",
    "    return analyzer.best_models, analyzer.cluster_labels, summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "from sklearn.metrics import (make_scorer, accuracy_score, precision_score, \n",
    "                           recall_score, f1_score, roc_auc_score, confusion_matrix)\n",
    "import time\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "class AdvancedModelManager:\n",
    "    \"\"\"\n",
    "    Comprehensive model management system for predictive maintenance\n",
    "    \"\"\"\n",
    "    def __init__(self, random_state: int = 42):\n",
    "        self.random_state = random_state\n",
    "        self.models = {}\n",
    "        self.results = {}\n",
    "        self.best_model = None\n",
    "        self.feature_importance = {}\n",
    "        \n",
    "    def add_model(self, name: str, model: BaseEstimator, params: Dict = None):\n",
    "        \"\"\"\n",
    "        Add a model to the manager with optional hyperparameters\n",
    "        \"\"\"\n",
    "        self.models[name] = {\n",
    "            'model': model,\n",
    "            'params': params or {},\n",
    "            'best_params': None,\n",
    "            'cv_results': None,\n",
    "            'trained_model': None\n",
    "        }\n",
    "        \n",
    "    def setup_default_models(self):\n",
    "        \"\"\"\n",
    "        Setup default models with optimized configurations for predictive maintenance\n",
    "        \"\"\"\n",
    "        from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "        from sklearn.neural_network import MLPClassifier\n",
    "        from sklearn.svm import SVC\n",
    "        from xgboost import XGBClassifier\n",
    "        from lightgbm import LGBMClassifier\n",
    "        \n",
    "        # Random Forest\n",
    "        rf_params = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [None, 10, 20, 30],\n",
    "            'min_samples_split': [2, 5, 10],\n",
    "            'class_weight': ['balanced', 'balanced_subsample']\n",
    "        }\n",
    "        self.add_model('RandomForest', \n",
    "                      RandomForestClassifier(random_state=self.random_state),\n",
    "                      rf_params)\n",
    "        \n",
    "        # Gradient Boosting\n",
    "        gb_params = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'learning_rate': [0.01, 0.1, 0.3],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'subsample': [0.8, 0.9, 1.0]\n",
    "        }\n",
    "        self.add_model('GradientBoosting',\n",
    "                      GradientBoostingClassifier(random_state=self.random_state),\n",
    "                      gb_params)\n",
    "        \n",
    "        # Neural Network\n",
    "        nn_params = {\n",
    "            'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
    "            'activation': ['relu', 'tanh'],\n",
    "            'learning_rate': ['adaptive'],\n",
    "            'max_iter': [300, 500]\n",
    "        }\n",
    "        self.add_model('NeuralNetwork',\n",
    "                      MLPClassifier(random_state=self.random_state),\n",
    "                      nn_params)\n",
    "        \n",
    "        # XGBoost\n",
    "        xgb_params = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.3],\n",
    "            'subsample': [0.8, 0.9, 1.0],\n",
    "            'scale_pos_weight': [1, 3, 5]\n",
    "        }\n",
    "        self.add_model('XGBoost',\n",
    "                      XGBClassifier(random_state=self.random_state),\n",
    "                      xgb_params)\n",
    "        \n",
    "        # LightGBM\n",
    "        lgb_params = {\n",
    "            'n_estimators': [100, 200, 300],\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'learning_rate': [0.01, 0.1, 0.3],\n",
    "            'subsample': [0.8, 0.9, 1.0],\n",
    "            'class_weight': ['balanced']\n",
    "        }\n",
    "        self.add_model('LightGBM',\n",
    "                      LGBMClassifier(random_state=self.random_state),\n",
    "                      lgb_params)\n",
    "        \n",
    "    def _create_custom_scorer(self):\n",
    "        \"\"\"\n",
    "        Create custom scoring metrics for model evaluation\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'accuracy': make_scorer(accuracy_score),\n",
    "            'precision': make_scorer(precision_score, average='weighted'),\n",
    "            'recall': make_scorer(recall_score, average='weighted'),\n",
    "            'f1': make_scorer(f1_score, average='weighted'),\n",
    "            'roc_auc': make_scorer(roc_auc_score, average='weighted',\n",
    "                                 needs_proba=True)\n",
    "        }\n",
    "        \n",
    "    def optimize_hyperparameters(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Perform hyperparameter optimization for all models\n",
    "        \"\"\"\n",
    "        from sklearn.model_selection import RandomizedSearchCV\n",
    "        \n",
    "        for name, model_info in self.models.items():\n",
    "            print(f\"\\nOptimizing {name}...\")\n",
    "            \n",
    "            # Create RandomizedSearchCV\n",
    "            random_search = RandomizedSearchCV(\n",
    "                estimator=model_info['model'],\n",
    "                param_distributions=model_info['params'],\n",
    "                n_iter=20,\n",
    "                cv=StratifiedKFold(n_splits=5, shuffle=True,\n",
    "                                 random_state=self.random_state),\n",
    "                scoring=self._create_custom_scorer(),\n",
    "                refit='f1',\n",
    "                random_state=self.random_state,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            \n",
    "            # Fit and store results\n",
    "            random_search.fit(X, y)\n",
    "            self.models[name]['best_params'] = random_search.best_params_\n",
    "            self.models[name]['cv_results'] = random_search.cv_results_\n",
    "            self.models[name]['trained_model'] = random_search.best_estimator_\n",
    "            \n",
    "    def evaluate_models(self, X: np.ndarray, y: np.ndarray):\n",
    "        \"\"\"\n",
    "        Evaluate all models using cross-validation\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for name, model_info in self.models.items():\n",
    "            if model_info['trained_model'] is None:\n",
    "                continue\n",
    "                \n",
    "            # Perform cross-validation\n",
    "            cv_results = cross_validate(\n",
    "                model_info['trained_model'],\n",
    "                X, y,\n",
    "                cv=StratifiedKFold(n_splits=5, shuffle=True,\n",
    "                                 random_state=self.random_state),\n",
    "                scoring=self._create_custom_scorer(),\n",
    "                return_train_score=True\n",
    "            )\n",
    "            \n",
    "            # Calculate and store results\n",
    "            mean_results = {\n",
    "                'Model': name,\n",
    "                'Test_Accuracy': cv_results['test_accuracy'].mean(),\n",
    "                'Test_Precision': cv_results['test_precision'].mean(),\n",
    "                'Test_Recall': cv_results['test_recall'].mean(),\n",
    "                'Test_F1': cv_results['test_f1'].mean(),\n",
    "                'Test_ROC_AUC': cv_results['test_roc_auc'].mean(),\n",
    "                'Train_Accuracy': cv_results['train_accuracy'].mean(),\n",
    "                'Fit_Time': cv_results['fit_time'].mean(),\n",
    "                'Score_Time': cv_results['score_time'].mean()\n",
    "            }\n",
    "            \n",
    "            results.append(mean_results)\n",
    "            \n",
    "        self.results = pd.DataFrame(results)\n",
    "        self._select_best_model()\n",
    "        \n",
    "    def _select_best_model(self):\n",
    "        \"\"\"\n",
    "        Select the best model based on F1 score\n",
    "        \"\"\"\n",
    "        best_idx = self.results['Test_F1'].idxmax()\n",
    "        self.best_model = self.models[self.results.loc[best_idx, 'Model']]['trained_model']\n",
    "        \n",
    "    def analyze_feature_importance(self, X: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Analyze feature importance for supported models\n",
    "        \"\"\"\n",
    "        for name, model_info in self.models.items():\n",
    "            model = model_info['trained_model']\n",
    "            \n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                self.feature_importance[name] = pd.Series(\n",
    "                    model.feature_importances_,\n",
    "                    index=X.columns\n",
    "                ).sort_values(ascending=False)\n",
    "            elif hasattr(model, 'coef_'):\n",
    "                self.feature_importance[name] = pd.Series(\n",
    "                    np.abs(model.coef_[0]),\n",
    "                    index=X.columns\n",
    "                ).sort_values(ascending=False)\n",
    "                \n",
    "    def plot_results(self, figsize=(15, 10)):\n",
    "        \"\"\"\n",
    "        Visualize model comparison results\n",
    "        \"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        import seaborn as sns\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "        \n",
    "        # Plot model metrics comparison\n",
    "        metrics = ['Test_Accuracy', 'Test_Precision', 'Test_Recall', 'Test_F1']\n",
    "        self.results[['Model'] + metrics].melt(\n",
    "            id_vars=['Model'],\n",
    "            value_vars=metrics\n",
    "        ).plot(kind='bar', x='Model', y='value', hue='variable',\n",
    "               ax=axes[0, 0])\n",
    "        axes[0, 0].set_title('Model Performance Comparison')\n",
    "        \n",
    "        # Plot training times\n",
    "        self.results[['Model', 'Fit_Time']].plot(\n",
    "            kind='bar', x='Model', y='Fit_Time',\n",
    "            ax=axes[0, 1])\n",
    "        axes[0, 1].set_title('Training Time Comparison')\n",
    "        \n",
    "        # Plot feature importance for best model\n",
    "        if self.feature_importance:\n",
    "            best_model_name = self.results.loc[\n",
    "                self.results['Test_F1'].idxmax(), 'Model'\n",
    "            ]\n",
    "            self.feature_importance[best_model_name].plot(\n",
    "                kind='bar', ax=axes[1, 0])\n",
    "            axes[1, 0].set_title(f'Feature Importance ({best_model_name})')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def get_prediction_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get probability predictions from best model\n",
    "        \"\"\"\n",
    "        if self.best_model is None:\n",
    "            raise ValueError(\"No best model selected. Run evaluate_models first.\")\n",
    "        return self.best_model.predict_proba(X)\n",
    "        \n",
    "    def get_feature_importance(self) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Get feature importance for all supported models\n",
    "        \"\"\"\n",
    "        return self.feature_importance\n",
    "\n",
    "# Usage example:\n",
    "def train_and_evaluate_models(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train and evaluate all models\n",
    "    \"\"\"\n",
    "    # Initialize model manager\n",
    "    model_manager = AdvancedModelManager()\n",
    "    \n",
    "    # Setup default models\n",
    "    model_manager.setup_default_models()\n",
    "    \n",
    "    # Optimize hyperparameters\n",
    "    model_manager.optimize_hyperparameters(X_train, y_train)\n",
    "    \n",
    "    # Evaluate models\n",
    "    model_manager.evaluate_models(X_train, y_train)\n",
    "    \n",
    "    # Analyze feature importance\n",
    "    if isinstance(X_train, pd.DataFrame):\n",
    "        model_manager.analyze_feature_importance(X_train)\n",
    "    \n",
    "    # Plot results\n",
    "    model_manager.plot_results()\n",
    "    \n",
    "    # Print detailed results\n",
    "    print(\"\\nModel Evaluation Results:\")\n",
    "    print(model_manager.results)\n",
    "    \n",
    "    return model_manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'shap'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlime\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlime\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlime_tabular\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'shap'"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Union, Optional\n",
    "from sklearn.base import BaseEstimator\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "class AdvancedModelInterpreter:\n",
    "    \"\"\"\n",
    "    Comprehensive model interpretation using multiple techniques and visualizations\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 model: BaseEstimator, \n",
    "                 X_train: np.ndarray,\n",
    "                 feature_names: List[str],\n",
    "                 class_names: List[str] = ['No Failure', 'Failure']):\n",
    "        \"\"\"\n",
    "        Initialize the interpreter\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model: trained model\n",
    "        X_train: training data\n",
    "        feature_names: list of feature names\n",
    "        class_names: list of class names\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.X_train = X_train\n",
    "        self.feature_names = feature_names\n",
    "        self.class_names = class_names\n",
    "        self.shap_explainer = None\n",
    "        self.lime_explainer = None\n",
    "        self.feature_importance = None\n",
    "        \n",
    "    def setup_explainers(self):\n",
    "        \"\"\"\n",
    "        Initialize SHAP and LIME explainers\n",
    "        \"\"\"\n",
    "        # Initialize SHAP explainer based on model type\n",
    "        if hasattr(self.model, 'apply'):  # Tree-based models\n",
    "            self.shap_explainer = shap.TreeExplainer(self.model)\n",
    "        else:  # Other models\n",
    "            background = shap.sample(self.X_train, 100)  # Sample background data\n",
    "            self.shap_explainer = shap.KernelExplainer(\n",
    "                self.model.predict_proba, background\n",
    "            )\n",
    "            \n",
    "        # Initialize LIME explainer\n",
    "        self.lime_explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "            self.X_train,\n",
    "            feature_names=self.feature_names,\n",
    "            class_names=self.class_names,\n",
    "            mode='classification'\n",
    "        )\n",
    "        \n",
    "    def explain_prediction(self, \n",
    "                         X: np.ndarray, \n",
    "                         idx: int = 0,\n",
    "                         method: str = 'both') -> Dict:\n",
    "        \"\"\"\n",
    "        Generate explanation for a single prediction\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: input data\n",
    "        idx: index of instance to explain\n",
    "        method: 'shap', 'lime', or 'both'\n",
    "        \"\"\"\n",
    "        if self.shap_explainer is None or self.lime_explainer is None:\n",
    "            self.setup_explainers()\n",
    "            \n",
    "        explanation = {}\n",
    "        \n",
    "        if method in ['shap', 'both']:\n",
    "            # Generate SHAP explanation\n",
    "            shap_values = self.shap_explainer.shap_values(X[idx:idx+1])\n",
    "            if isinstance(shap_values, list):\n",
    "                shap_values = shap_values[1]  # For tree-based models\n",
    "                \n",
    "            explanation['shap'] = {\n",
    "                'values': shap_values,\n",
    "                'base_value': self.shap_explainer.expected_value if not isinstance(\n",
    "                    self.shap_explainer.expected_value, list) \n",
    "                    else self.shap_explainer.expected_value[1]\n",
    "            }\n",
    "            \n",
    "        if method in ['lime', 'both']:\n",
    "            # Generate LIME explanation\n",
    "            lime_exp = self.lime_explainer.explain_instance(\n",
    "                X[idx], \n",
    "                self.model.predict_proba,\n",
    "                num_features=len(self.feature_names)\n",
    "            )\n",
    "            explanation['lime'] = lime_exp\n",
    "            \n",
    "        return explanation\n",
    "    \n",
    "    def plot_feature_importance(self, \n",
    "                              plot_type: str = 'shap',\n",
    "                              top_n: Optional[int] = None):\n",
    "        \"\"\"\n",
    "        Plot global feature importance\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        plot_type: 'shap' or 'permutation'\n",
    "        top_n: number of top features to show\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        if plot_type == 'shap':\n",
    "            if self.shap_explainer is None:\n",
    "                self.setup_explainers()\n",
    "                \n",
    "            # Calculate SHAP values for all instances\n",
    "            shap_values = self.shap_explainer.shap_values(self.X_train)\n",
    "            if isinstance(shap_values, list):\n",
    "                shap_values = shap_values[1]\n",
    "                \n",
    "            # Plot SHAP summary\n",
    "            shap.summary_plot(\n",
    "                shap_values, \n",
    "                self.X_train,\n",
    "                feature_names=self.feature_names,\n",
    "                plot_type='bar',\n",
    "                max_display=top_n\n",
    "            )\n",
    "        else:\n",
    "            # Calculate permutation importance\n",
    "            from sklearn.inspection import permutation_importance\n",
    "            perm_importance = permutation_importance(\n",
    "                self.model, self.X_train, \n",
    "                self.model.predict(self.X_train),\n",
    "                n_repeats=10\n",
    "            )\n",
    "            \n",
    "            # Create importance DataFrame\n",
    "            importance_df = pd.DataFrame({\n",
    "                'feature': self.feature_names,\n",
    "                'importance': perm_importance.importances_mean\n",
    "            }).sort_values('importance', ascending=False)\n",
    "            \n",
    "            if top_n:\n",
    "                importance_df = importance_df.head(top_n)\n",
    "                \n",
    "            # Plot permutation importance\n",
    "            sns.barplot(\n",
    "                data=importance_df,\n",
    "                x='importance',\n",
    "                y='feature'\n",
    "            )\n",
    "            plt.title('Permutation Feature Importance')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_feature_interactions(self, \n",
    "                                X: np.ndarray,\n",
    "                                feature1: str,\n",
    "                                feature2: str):\n",
    "        \"\"\"\n",
    "        Plot interaction effects between two features\n",
    "        \"\"\"\n",
    "        if self.shap_explainer is None:\n",
    "            self.setup_explainers()\n",
    "            \n",
    "        shap_values = self.shap_explainer.shap_values(X)\n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = shap_values[1]\n",
    "            \n",
    "        # Get feature indices\n",
    "        feat1_idx = self.feature_names.index(feature1)\n",
    "        feat2_idx = self.feature_names.index(feature2)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        shap.dependence_plot(\n",
    "            feat1_idx,\n",
    "            shap_values,\n",
    "            X,\n",
    "            interaction_index=feat2_idx,\n",
    "            feature_names=self.feature_names\n",
    "        )\n",
    "        plt.title(f'Interaction between {feature1} and {feature2}')\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_decision_boundary(self, \n",
    "                             X: np.ndarray,\n",
    "                             feature1: str,\n",
    "                             feature2: str):\n",
    "        \"\"\"\n",
    "        Plot decision boundary for two features\n",
    "        \"\"\"\n",
    "        # Get feature indices\n",
    "        feat1_idx = self.feature_names.index(feature1)\n",
    "        feat2_idx = self.feature_names.index(feature2)\n",
    "        \n",
    "        # Create mesh grid\n",
    "        x1_min, x1_max = X[:, feat1_idx].min() - 1, X[:, feat1_idx].max() + 1\n",
    "        x2_min, x2_max = X[:, feat2_idx].min() - 1, X[:, feat2_idx].max() + 1\n",
    "        xx1, xx2 = np.meshgrid(\n",
    "            np.arange(x1_min, x1_max, (x1_max - x1_min) / 100),\n",
    "            np.arange(x2_min, x2_max, (x2_max - x2_min) / 100)\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        X_mesh = np.zeros((xx1.ravel().shape[0], X.shape[1]))\n",
    "        X_mesh[:, feat1_idx] = xx1.ravel()\n",
    "        X_mesh[:, feat2_idx] = xx2.ravel()\n",
    "        Z = self.model.predict(X_mesh)\n",
    "        Z = Z.reshape(xx1.shape)\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.contourf(xx1, xx2, Z, alpha=0.4)\n",
    "        plt.scatter(X[:, feat1_idx], X[:, feat2_idx], \n",
    "                   c=self.model.predict(X), alpha=0.8)\n",
    "        plt.xlabel(feature1)\n",
    "        plt.ylabel(feature2)\n",
    "        plt.title('Decision Boundary')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        \n",
    "    def plot_prediction_explanation(self, \n",
    "                                  X: np.ndarray,\n",
    "                                  idx: int = 0):\n",
    "        \"\"\"\n",
    "        Plot detailed explanation for a single prediction\n",
    "        \"\"\"\n",
    "        explanation = self.explain_prediction(X, idx)\n",
    "        \n",
    "        # Create subplots\n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 12))\n",
    "        \n",
    "        # Plot SHAP explanation\n",
    "        if 'shap' in explanation:\n",
    "            shap_values = explanation['shap']['values']\n",
    "            base_value = explanation['shap']['base_value']\n",
    "            \n",
    "            # Waterfall plot\n",
    "            shap.plots._waterfall.waterfall_plot(\n",
    "                base_value,\n",
    "                shap_values[0],\n",
    "                feature_names=self.feature_names,\n",
    "                ax=ax1\n",
    "            )\n",
    "            ax1.set_title('SHAP Feature Contributions')\n",
    "            \n",
    "        # Plot LIME explanation\n",
    "        if 'lime' in explanation:\n",
    "            lime_exp = explanation['lime']\n",
    "            lime_exp.as_pyplot_figure(ax=ax2)\n",
    "            ax2.set_title('LIME Feature Contributions')\n",
    "            \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def get_critical_features(self, \n",
    "                            X: np.ndarray,\n",
    "                            threshold: float = 0.1) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Identify critical features for predictions\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X: input data\n",
    "        threshold: importance threshold\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        Dictionary of positive and negative influential features\n",
    "        \"\"\"\n",
    "        if self.shap_explainer is None:\n",
    "            self.setup_explainers()\n",
    "            \n",
    "        shap_values = self.shap_explainer.shap_values(X)\n",
    "        if isinstance(shap_values, list):\n",
    "            shap_values = shap_values[1]\n",
    "            \n",
    "        # Calculate mean absolute SHAP values\n",
    "        mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "        \n",
    "        # Identify critical features\n",
    "        critical_features = {\n",
    "            'positive': [],\n",
    "            'negative': []\n",
    "        }\n",
    "        \n",
    "        for idx, importance in enumerate(mean_abs_shap):\n",
    "            if importance > threshold:\n",
    "                if np.mean(shap_values[:, idx]) > 0:\n",
    "                    critical_features['positive'].append(self.feature_names[idx])\n",
    "                else:\n",
    "                    critical_features['negative'].append(self.feature_names[idx])\n",
    "                    \n",
    "        return critical_features\n",
    "\n",
    "# Usage example:\n",
    "def interpret_model(model, X_train, X_test, feature_names):\n",
    "    \"\"\"\n",
    "    Generate comprehensive model interpretation\n",
    "    \"\"\"\n",
    "    # Initialize interpreter\n",
    "    interpreter = AdvancedModelInterpreter(\n",
    "        model=model,\n",
    "        X_train=X_train,\n",
    "        feature_names=feature_names\n",
    "    )\n",
    "    \n",
    "    # Generate global feature importance\n",
    "    print(\"Generating feature importance plots...\")\n",
    "    interpreter.plot_feature_importance(plot_type='shap')\n",
    "    interpreter.plot_feature_importance(plot_type='permutation')\n",
    "    \n",
    "    # Generate local explanation for a sample\n",
    "    print(\"\\nGenerating local explanation for a sample prediction...\")\n",
    "    interpreter.plot_prediction_explanation(X_test, idx=0)\n",
    "    \n",
    "    # Identify critical features\n",
    "    print(\"\\nIdentifying critical features...\")\n",
    "    critical_features = interpreter.get_critical_features(X_test)\n",
    "    print(\"Positive influential features:\", critical_features['positive'])\n",
    "    print(\"Negative influential features:\", critical_features['negative'])\n",
    "    \n",
    "    # Generate feature interactions for top features\n",
    "    if len(critical_features['positive']) >= 2:\n",
    "        print(\"\\nGenerating feature interaction plot...\")\n",
    "        interpreter.plot_feature_interactions(\n",
    "            X_test,\n",
    "            critical_features['positive'][0],\n",
    "            critical_features['positive'][1]\n",
    "        )\n",
    "    \n",
    "    return interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
